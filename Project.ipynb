{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score,confusion_matrix,classification_report\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "import string\n",
    "\n",
    "import nltk\n",
    "from nltk import pos_tag\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.util import ngrams\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  generated\n",
      "0  Cars. Cars have been around since they became ...        0.0\n",
      "1  Transportation is a large necessity in most co...        0.0\n",
      "2  \"America's love affair with it's vehicles seem...        0.0\n",
      "3  How often do you ride in a car? Do you drive a...        0.0\n",
      "4  Cars are a wonderful thing. They are perhaps o...        0.0\n",
      "                                                     text  generated\n",
      "487230  Tie Face on Mars is really just a big misunder...        0.0\n",
      "487231  The whole purpose of democracy is to create a ...        0.0\n",
      "487232  I firmly believe that governments worldwide sh...        1.0\n",
      "487233  I DFN't agree with this decision because a LFT...        0.0\n",
      "487234  Richard Non, Jimmy Carter, and Bob Dole and ot...        0.0\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('AI_Human.csv')\n",
    "print(data.head())\n",
    "print(data.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>generated</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>This argument of the face on Mars been going o...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>What happen on Mars. In May 24, 2001. One of o...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>The face on Mars wasn't created by aliens, in ...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>40 years ago, on mars, NASA caught a picture o...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>The landform on Mars is very similar to the me...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  generated\n",
       "9995  This argument of the face on Mars been going o...        0.0\n",
       "9996  What happen on Mars. In May 24, 2001. One of o...        0.0\n",
       "9997  The face on Mars wasn't created by aliens, in ...        0.0\n",
       "9998  40 years ago, on mars, NASA caught a picture o...        0.0\n",
       "9999  The landform on Mars is very similar to the me...        0.0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data[:10000]\n",
    "data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "8964\n",
      "1036\n",
      "Transportation is a large necessity in most countries worldwide. With no doubt, cars, buses, and other means of transportation make going from place to place easier and faster. However there's always a negative pollution. Although mobile transportation are a huge part of daily lives, we are endangering the Earth with harmful greenhouse gases, which could be suppressed.\n",
      "\n",
      "A small suburb community in Germany called Vauban, has started a \"carfree\" lifestyle. In this city, markets and stores are placed nearby homes, instead of being located by farend highways. Although Vauban is not completely carfree, 70% of Vauban families do not own cars Even a large 57% of families stated to have sold their cars to move to Vauban. Some families have even said to be less stressed depending on car transportation. Cars are responsible for about 12% of greenhouse gases, and can even be up to 50% in some carintensive areas in the United States.\n",
      "\n",
      "Another insight to reduced car zones brings Paris' incident with smog. Paris' officials created a system that would in fact lower smog rates. On Monday, the motorists with evennumbered license plates numbers would be ordered to leave their cars at home, or they would suffer a fine. Same rule would occur on Tuesday, except motorists with oddnumbered license plates were targeted with fines. Congestion, or traffic, was reduced by 60% after five days of intense smog. Diesel fuel played a huge part in this pollution, having the fact that 67% of vehicles in France are of Diesel fuel. The impact of the clearing of smog, resided in banning the Tuesday rule of odd license plates.\n",
      "\n",
      "Could you imagine a day without seeing a single car being used? This phenomenon occurs once a year in Bogota, Colombia. With the exception of buses and taxis being used, cars are to be left unattended for an entire day. Having a carfree day just once a year can even reduce the pollution slightly. The day without cars is part of a campaign that originated in Bogota in the mid 1990s. This campaign has renewed and constructed numerous bicycle paths and sidewalks all over the city. Parks and sports centers have also sprung from this campaign. Devoting your time to a carfree lifestyle has it's hassles, but in hindsight, it has it's benefits.\n",
      "\n",
      "To conclude, living a carfree lifestyle does not seem like a possibility in this day and age, however managing the use of cars and pollution is something every country should take time investing in. Think about how much of an impact it would be if everywhere worldwide would take part in airpollution reduction. Mobile transportation is lifestyle in a sense, and being dependent on cars or other means of transportation can impact the health of the Earth and even ourselves.\n"
     ]
    }
   ],
   "source": [
    "print((data['generated']).count())\n",
    "print((data['generated'] == 0.0).sum())\n",
    "print((data['generated'] == 1.0).sum())\n",
    "print(data['text'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preprocessing\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# def preprocessText(text):\n",
    "#     text = ' '.join(text)\n",
    "#     text = text.lower()\n",
    "#     text = [word for word in text if word not in string.punctuation]\n",
    "#     text = ''.join(text)\n",
    "#     tokens = [token for token in text if token not in stop_words]\n",
    "#     tokens = word_tokenize(text)\n",
    "#     print(tokens)\n",
    "#     # lemmatizedTokens = [lemmatizer.lemmatize(word) for word in text if word not in stop_words]\n",
    "#     preprocessedText = ' '.join(tokens)\n",
    "#     return preprocessedText\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    tokens = word_tokenize(text)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    preprocessed_text = ' '.join(tokens)\n",
    "    \n",
    "    return preprocessed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'haven', 'can', 'them', 'himself', 'so', 'other', 'than', 'where', 'hers', \"hasn't\", 'needn', 'itself', \"shan't\", \"wouldn't\", 'your', 'myself', 'very', 'themselves', 'an', 'below', 'ain', 'for', 'will', 'before', 'too', 'here', 'herself', 'being', 'theirs', 'yourself', 'did', 'same', 'was', 'wasn', 'been', 'ours', 'both', \"aren't\", 'when', 'after', \"it's\", 'that', 'about', 'but', 'do', 'shouldn', 'once', 'didn', 'further', \"you'll\", \"that'll\", 'it', 'the', 'and', 'down', 'each', 'which', 'only', 'don', 'won', 'his', 'these', 'have', 'at', 'should', 'there', 'has', 'off', 'm', 'yourselves', 'on', 'any', 'mightn', \"wasn't\", 'i', 'just', 'my', \"you'd\", 'this', 'couldn', 'again', 'not', 'out', 's', 'her', 'against', 'how', \"you're\", 'ma', 'until', 'yours', 'of', 'we', 'all', \"she's\", 'now', 'a', 'its', 'him', 'few', 'their', 've', 'then', 'most', 'am', 'nor', 'above', 'll', 'those', 'you', 'through', \"haven't\", 'd', 'she', 'why', 'aren', 'to', \"should've\", 'had', 'as', 'were', \"won't\", 're', 'are', 'does', 'wouldn', 'me', 'having', 'with', \"didn't\", 'they', 'our', 'doing', 'o', 'up', 't', 'because', 'while', 'y', 'hasn', 'isn', 'by', 'during', \"you've\", \"mightn't\", \"weren't\", 'between', 'he', 'into', 'is', 'under', 'doesn', \"mustn't\", 'some', \"shouldn't\", 'or', 'over', 'who', \"needn't\", 'in', 'ourselves', \"isn't\", 'weren', 'be', 'mustn', 'whom', \"hadn't\", \"couldn't\", 'more', 'hadn', 'such', 'shan', 'no', \"don't\", 'from', 'if', 'what', 'own', \"doesn't\"}\n",
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
     ]
    }
   ],
   "source": [
    "print(stop_words)\n",
    "print(string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Preprocessed Text'] = data['text'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cars cars around since became famous 1900s henry ford created built first modelt cars played major role every day lives since people starting question limiting car usage would good thing limiting use cars might good thing like matter article german suburb life goes without cars elizabeth rosenthal states automobiles linchpin suburbs middle class families either shanghai chicago tend make homes experts say huge impediment current efforts reduce greenhouse gas emissions tailpipe passenger cars responsible 12 percent greenhouse gas emissions europeand 50 percent carintensive areas united states cars main reason greenhouse gas emissions lot people driving around time getting need go article paris bans driving due smog robert duffer says paris days nearrecord pollution enforced partial driving ban clear air global city also says monday motorist evennumbered license plates ordered leave cars home fined 22euro fine 31 order would applied oddnumbered plates following day cars reason polluting entire cities like paris shows bad cars pollution cause entire city likewise article carfree day spinning big hit bogota andrew selsky says programs thats set spread countries millions columbians hiked biked skated took bus work carfree day leaving streets capital city eerily devoid traffic jams third straight year cars banned buses taxis permitted day without cars capital city 7 million people like idea carfree days allows lesson pollution cars put exhaust people driving time article also tells parks sports centers bustled throughout city uneven pitted sidewalks replaced broad smooth sidewalks rushhour restrictions dramatically cut traffic new restaurants upscale shopping districts cropped cars good country columbia aloud repair things needed repairs long time traffic jams gone restaurants shopping districts popped due fact less cars around conclusion use less cars carfree days big impact environment cities cutting air pollution cars majorly polluted aloud countries like columbia repair sidewalks cut traffic jams limiting use cars would good thing america limit use cars maybe riding bike maybe walking somewhere isnt far doesnt need use car get limiting use cars might good thing'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Preprocessed Text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_features(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    pos_tags = [tag for word, tag in pos_tag(tokens)]\n",
    "    return pos_tags\n",
    "\n",
    "data['pos_features'] = data['Preprocessed Text'].apply(pos_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "X_word_freq = vectorizer.fit_transform(data['Preprocessed Text'])\n",
    "word_freq_feature_names = vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_ngrams(text, n):\n",
    "    tokens = word_tokenize(text)\n",
    "    ngrams_list = list(ngrams(tokens, n))\n",
    "    return [' '.join(gram) for gram in ngrams_list]\n",
    "\n",
    "data['ngrams'] = data['Preprocessed Text'].apply(lambda x: extract_ngrams(x, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pos = nltk.pos_tag_sents(data['Preprocessed Text'].apply(word_tokenize))\n",
    "# X_combined = []\n",
    "# for i in range(len(data)):\n",
    "#     combined_features = list(data['pos_features'][i]) + list(data['ngrams'][i])\n",
    "#     X_combined.append(combined_features)\n",
    "\n",
    "X_combined = data[['pos_features', 'ngrams']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                           pos_features  \\\n",
      "0     [NNS, NNS, IN, IN, VBD, JJ, CD, NN, NN, VBD, V...   \n",
      "1     [NN, JJ, NN, NNS, VBP, NN, NNS, VBZ, VBZ, NN, ...   \n",
      "2     [RB, VB, NN, NNS, VBZ, VBG, VBZ, NN, NN, NN, N...   \n",
      "3     [RB, JJ, NN, NN, CD, NN, NN, NN, NN, NN, RB, V...   \n",
      "4     [NNS, JJ, NN, RB, CD, NNS, JJS, NNS, NNS, NNS,...   \n",
      "...                                                 ...   \n",
      "9995  [NN, NN, VBZ, VBG, IN, CD, JJ, NNS, RB, JJS, V...   \n",
      "9996  [JJ, NNS, MD, CD, CD, NN, NN, VBD, JJ, NNS, JJ...   \n",
      "9997  [NN, NNS, VBP, VBN, NNS, NN, NN, JJ, NN, NN, N...   \n",
      "9998  [CD, NNS, RB, NNS, RB, VBD, NN, JJ, NN, NN, VB...   \n",
      "9999  [NN, NNS, JJ, NNS, NNS, RB, VBP, JJS, NN, NNS,...   \n",
      "\n",
      "                                                 ngrams  \n",
      "0     [cars cars, cars around, around since, since b...  \n",
      "1     [transportation large, large necessity, necess...  \n",
      "2     [americas love, love affair, affair vehicles, ...  \n",
      "3     [often ride, ride car, car drive, drive one, o...  \n",
      "4     [cars wonderful, wonderful thing, thing perhap...  \n",
      "...                                                 ...  \n",
      "9995  [argument face, face mars, mars going, going s...  \n",
      "9996  [happen mars, mars may, may 24, 24 2001, 2001 ...  \n",
      "9997  [face mars, mars wasnt, wasnt created, created...  \n",
      "9998  [40 years, years ago, ago mars, mars nasa, nas...  \n",
      "9999  [landform mars, mars similar, similar mesas, m...  \n",
      "\n",
      "[10000 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "print(X_combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data['Preprocessed Text']\n",
    "y = data['generated']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000\n",
      "8000\n"
     ]
    }
   ],
   "source": [
    "print(len(X_train))\n",
    "print(len(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;count_vectorizer&#x27;, CountVectorizer()),\n",
       "                (&#x27;tfidf_transformer&#x27;, TfidfTransformer()),\n",
       "                (&#x27;naive_bayes&#x27;, MultinomialNB())])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;count_vectorizer&#x27;, CountVectorizer()),\n",
       "                (&#x27;tfidf_transformer&#x27;, TfidfTransformer()),\n",
       "                (&#x27;naive_bayes&#x27;, MultinomialNB())])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">CountVectorizer</label><div class=\"sk-toggleable__content\"><pre>CountVectorizer()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TfidfTransformer</label><div class=\"sk-toggleable__content\"><pre>TfidfTransformer()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MultinomialNB</label><div class=\"sk-toggleable__content\"><pre>MultinomialNB()</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('count_vectorizer', CountVectorizer()),\n",
       "                ('tfidf_transformer', TfidfTransformer()),\n",
       "                ('naive_bayes', MultinomialNB())])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('count_vectorizer', CountVectorizer()), \n",
    "    ('tfidf_transformer', TfidfTransformer()),  \n",
    "    ('naive_bayes', MultinomialNB())])\n",
    "\n",
    "pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.97      1.00      0.99      1795\n",
      "         1.0       1.00      0.76      0.86       205\n",
      "\n",
      "    accuracy                           0.98      2000\n",
      "   macro avg       0.99      0.88      0.93      2000\n",
      "weighted avg       0.98      0.98      0.97      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred= pipeline.predict(X_test)\n",
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pos_str = [[' '.join(tag) for tag in sentence] for sentence in X_pos]\n",
    "\n",
    "X_combined_text = [' '.join(pos_features) + ' ' + ' '.join(ngrams) for pos_features, ngrams in zip(X_pos_str, data['ngrams'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_combined_vectorized = vectorizer.fit_transform(X_combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Feature Extraction\n",
    "X_flat = [' '.join(row['pos_features']) + ' ' + ' '.join(row['ngrams']) for index, row in X_combined.iterrows()]\n",
    "X_flat_vectorized = vectorizer.fit_transform(X_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.983\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_flat_vectorized, y, test_size=0.2, random_state=42)\n",
    "model = MultinomialNB()\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "accuracy = model.score(X_test, y_test)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of combinedFeaturesVectorizer: (1, 171)\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of combinedFeaturesVectorizer:\", combinedFeaturesVectorizer.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "X has 171 features, but MultinomialNB is expecting 50656 features as input.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[107], line 23\u001b[0m\n\u001b[0;32m     21\u001b[0m combined_features \u001b[38;5;241m=\u001b[39m pos_tags \u001b[38;5;241m+\u001b[39m bi_grams_str \u001b[38;5;241m+\u001b[39m tri_grams_str\n\u001b[0;32m     22\u001b[0m test_sample_vectorized \u001b[38;5;241m=\u001b[39m vectorizer\u001b[38;5;241m.\u001b[39mtransform([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(combined_features)])\n\u001b[1;32m---> 23\u001b[0m prediction \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_sample_vectorized\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPredicted label:\u001b[39m\u001b[38;5;124m\"\u001b[39m, prediction)\n",
      "File \u001b[1;32mc:\\libraries\\python\\lib\\site-packages\\sklearn\\naive_bayes.py:101\u001b[0m, in \u001b[0;36m_BaseNB.predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     88\u001b[0m \u001b[38;5;124;03mPerform classification on an array of test vectors X.\u001b[39;00m\n\u001b[0;32m     89\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     98\u001b[0m \u001b[38;5;124;03m    Predicted target values for X.\u001b[39;00m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    100\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m--> 101\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_X\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    102\u001b[0m jll \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_joint_log_likelihood(X)\n\u001b[0;32m    103\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_[np\u001b[38;5;241m.\u001b[39margmax(jll, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)]\n",
      "File \u001b[1;32mc:\\libraries\\python\\lib\\site-packages\\sklearn\\naive_bayes.py:574\u001b[0m, in \u001b[0;36m_BaseDiscreteNB._check_X\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    572\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_check_X\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[0;32m    573\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Validate X, used only in predict* methods.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 574\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\libraries\\python\\lib\\site-packages\\sklearn\\base.py:625\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[0;32m    622\u001b[0m     out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[0;32m    624\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mensure_2d\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m--> 625\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_n_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    627\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[1;32mc:\\libraries\\python\\lib\\site-packages\\sklearn\\base.py:414\u001b[0m, in \u001b[0;36mBaseEstimator._check_n_features\u001b[1;34m(self, X, reset)\u001b[0m\n\u001b[0;32m    411\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m    413\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_features \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_features_in_:\n\u001b[1;32m--> 414\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    415\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX has \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_features\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m features, but \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    416\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis expecting \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_features_in_\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m features as input.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    417\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: X has 171 features, but MultinomialNB is expecting 50656 features as input."
     ]
    }
   ],
   "source": [
    "testText = data['Preprocessed Text'][145]\n",
    "# posTags = []\n",
    "# ngramsList = []\n",
    "# posTags = pos_features(testText)\n",
    "# ngramsList = extract_ngrams(testText,2)\n",
    "# combinedFeatures = posTags + ngramsList\n",
    "# print(combinedFeatures)\n",
    "# combinedFeaturesVectorizer = vectorizer.transform([' '.join(combinedFeatures)])\n",
    "# predictedLabel = model.predict(combinedFeaturesVectorizer)\n",
    "# print(f'Predicted Label:{predictedLabel[0]}')\n",
    "\n",
    "pos_tags = [tag for _, tag in pos_tag(word_tokenize(testText))]\n",
    "\n",
    "tokens = word_tokenize(testText)\n",
    "bi_grams = list(ngrams(tokens, 2))  \n",
    "tri_grams = list(ngrams(tokens, 3))  \n",
    "\n",
    "bi_grams_str = [' '.join(gram) for gram in bi_grams]\n",
    "tri_grams_str = [' '.join(gram) for gram in tri_grams]\n",
    "\n",
    "combined_features = pos_tags + bi_grams_str + tri_grams_str\n",
    "test_sample_vectorized = vectorizer.transform([' '.join(combined_features)])\n",
    "prediction = model.predict(test_sample_vectorized)\n",
    "print(\"Predicted label:\", prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of test_sample_vectorized: (1, 171)\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of test_sample_vectorized:\", test_sample_vectorized.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_combined: 10000\n",
      "Shape of y: 10000\n",
      "Contents of X_combined:                                            pos_features  \\\n",
      "0     [NNS, NNS, IN, IN, VBD, JJ, CD, NN, NN, VBD, V...   \n",
      "1     [NN, JJ, NN, NNS, VBP, NN, NNS, VBZ, VBZ, NN, ...   \n",
      "2     [RB, VB, NN, NNS, VBZ, VBG, VBZ, NN, NN, NN, N...   \n",
      "3     [RB, JJ, NN, NN, CD, NN, NN, NN, NN, NN, RB, V...   \n",
      "4     [NNS, JJ, NN, RB, CD, NNS, JJS, NNS, NNS, NNS,...   \n",
      "...                                                 ...   \n",
      "9995  [NN, NN, VBZ, VBG, IN, CD, JJ, NNS, RB, JJS, V...   \n",
      "9996  [JJ, NNS, MD, CD, CD, NN, NN, VBD, JJ, NNS, JJ...   \n",
      "9997  [NN, NNS, VBP, VBN, NNS, NN, NN, JJ, NN, NN, N...   \n",
      "9998  [CD, NNS, RB, NNS, RB, VBD, NN, JJ, NN, NN, VB...   \n",
      "9999  [NN, NNS, JJ, NNS, NNS, RB, VBP, JJS, NN, NNS,...   \n",
      "\n",
      "                                                 ngrams  \n",
      "0     [cars cars, cars around, around since, since b...  \n",
      "1     [transportation large, large necessity, necess...  \n",
      "2     [americas love, love affair, affair vehicles, ...  \n",
      "3     [often ride, ride car, car drive, drive one, o...  \n",
      "4     [cars wonderful, wonderful thing, thing perhap...  \n",
      "...                                                 ...  \n",
      "9995  [argument face, face mars, mars going, going s...  \n",
      "9996  [happen mars, mars may, may 24, 24 2001, 2001 ...  \n",
      "9997  [face mars, mars wasnt, wasnt created, created...  \n",
      "9998  [40 years, years ago, ago mars, mars nasa, nas...  \n",
      "9999  [landform mars, mars similar, similar mesas, m...  \n",
      "\n",
      "[10000 rows x 2 columns]\n",
      "Contents of y: 0       0.0\n",
      "1       0.0\n",
      "2       0.0\n",
      "3       0.0\n",
      "4       0.0\n",
      "       ... \n",
      "9995    0.0\n",
      "9996    0.0\n",
      "9997    0.0\n",
      "9998    0.0\n",
      "9999    0.0\n",
      "Name: generated, Length: 10000, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of X_combined:\", len(X_combined))\n",
    "print(\"Shape of y:\", len(y))\n",
    "\n",
    "# Print the contents of X_combined and y\n",
    "print(\"Contents of X_combined:\", X_combined)\n",
    "print(\"Contents of y:\", y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
